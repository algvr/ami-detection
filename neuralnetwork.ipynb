{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
    "import tensorflow as tf\n",
    "print(tf.version.VERSION)    \n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import IPython\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from ecg_plotting import *\n",
    "# from IPython.core.display_functions import display\n",
    "import numpy as np\n",
    "# download datasets\n",
    "\n",
    "from dataset_downloader import download_dataset\n",
    "import datasets.ptb_xl.data_handling as ptb_xl_dh\n",
    "\n",
    "download_dataset('backgrounds')\n",
    "download_dataset('ptb_xl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing and Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.ptb_xl.data_handling import get_ecg_array\n",
    "Nsamples = 100\n",
    "X, Ylabel = get_ecg_array(sampling_rate=500,max_samples=Nsamples)\n",
    "X = np.reshape(X[:,452:-452,:],(-1, 4096,12 ,1))\n",
    "Y = np.zeros((Nsamples))\n",
    "\n",
    "dangerList = [\"IMI\",\"ASMI\",\"ILMI\",\"AMI\",\"LMI\",\"IPLMI\",\"IPMI\",\"PMI\"]\n",
    "for k in range(Nsamples):\n",
    "    if len(set(Ylabel['scp_codes'].iloc[k].keys()) & set(dangerList)):\n",
    "        Y[k] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(layers.Layer):\n",
    "    def __init__(self, LastNumFilters,**kwargs):\n",
    "        super(Residual, self).__init__(**kwargs)\n",
    "        self.LastNumFilters = LastNumFilters\n",
    "        \n",
    "        self.layer_1 = layers.Conv2D(LastNumFilters+64, (16, 1), activation=None, padding='same')\n",
    "        self.layer_2 = layers.MaxPooling2D((2, 1)) # CHECK BECAUSE PAPER DOESN'T MENTION PRECISELY\n",
    "        self.layer_3 = layers.BatchNormalization(axis=[1,2]) #? axis = [1,2] to normalize the axis=0 (over the batch)\n",
    "        self.layer_4 = layers.Activation(activation='relu')\n",
    "        self.layer_5 = layers.Dropout(dropOutP) # To prevent overfit\n",
    "        self.layer_6 = layers.Conv2D(LastNumFilters+64, (16, 1), activation=None, padding='same')\n",
    "        self.layer_7 = layers.MaxPooling2D((2, 1)) # CHECK BECAUSE PAPER DOESN'T MENTION PRECISELY\n",
    "        self.layer_8 = layers.BatchNormalization(axis=[1,2]) #? axis = [1,2] to normalize the axis=0 (over the batch)\n",
    "        self.layer_9 = layers.Activation(activation='relu')\n",
    "        self.layer_10 = layers.Dropout(dropOutP) # To prevent overfit\n",
    "\n",
    "        self.layer_11 = layers.Conv2D(LastNumFilters+64, (1, 1), activation=None, padding='same')\n",
    "        self.layer_12 = layers.MaxPooling2D((4, 1)) # CHECK BECAUSE PAPER DOESN'T MENTION PRECISELY\n",
    "        self.layer_13 = layers.BatchNormalization(axis=[1,2]) #? axis = [1,2] to normalize the axis=0 (over the batch)\n",
    "        self.layer_14 = layers.Activation(activation='relu')\n",
    "        self.layer_15 = layers.Dropout(dropOutP) # To prevent overfit\n",
    "        \n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        # the residual block using Keras functional API\n",
    "        xbackup = x\n",
    "        LastNumFilters = self.LastNumFilters\n",
    "\n",
    "        x = self.layer_1(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        x = self.layer_4(x)\n",
    "        x = self.layer_5(x)\n",
    "        x = self.layer_6(x)\n",
    "        x = self.layer_7(x)\n",
    "        x = self.layer_8(x)\n",
    "        x = self.layer_9(x)\n",
    "        x = self.layer_10(x)\n",
    "\n",
    "        xbackup = self.layer_11(xbackup)\n",
    "        xbackup = self.layer_12(xbackup)\n",
    "        xbackup = self.layer_13(xbackup)\n",
    "        xbackup = self.layer_14(xbackup)                                                           \n",
    "        xbackup = self.layer_15(xbackup)\n",
    "        \n",
    "        x = layers.Add()([x,xbackup])\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Construct the model\n",
    "dropOutP = 0.1\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(64, (16, 1), activation=None, input_shape=(4096, 12, 1)))\n",
    "model.add(layers.BatchNormalization(axis=[1,2])) #? axis = [1,2] to normalize the axis=0 (over the batch)\n",
    "model.add(layers.Activation(activation='relu'))\n",
    "model.add(layers.Dropout(dropOutP)) # To prevent overfit\n",
    "\n",
    "lastNumFilters = 64\n",
    "model.add(Residual(lastNumFilters))\n",
    "lastNumFilters += 64\n",
    "model.add(Residual(lastNumFilters))\n",
    "lastNumFilters += 64\n",
    "model.add(Residual(lastNumFilters))\n",
    "lastNumFilters += 64\n",
    "model.add(Residual(lastNumFilters))\n",
    "lastNumFilters += 64\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, Y, epochs=100, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e8fe47fbcf0df5fb1553eccd10fe68adaaecd5a77378650c47b51c76c16f447a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
