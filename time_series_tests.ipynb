{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from ecg_plotting import *\n",
    "import IPython\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, initializers"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# download datasets\n",
    "\n",
    "from dataset_downloader import download_dataset\n",
    "import datasets.ptb_xl.data_handling as ptb_xl_dh\n",
    "\n",
    "download_dataset('backgrounds')\n",
    "download_dataset('ptb_xl')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing and Labeling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Assign labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from datasets.ptb_xl.data_handling import get_ecg_array\n",
    "\n",
    "num_samples = 5000\n",
    "X, Y_label = get_ecg_array(sampling_rate=500, max_samples=num_samples)\n",
    "X = np.reshape(X[:,452:-452,:],(-1, 4096,12 ,1))\n",
    "Y = np.zeros((num_samples))\n",
    "\n",
    "danger_list = [\"IMI\", \"ASMI\", \"ILMI\", \"AMI\", \"LMI\", \"IPLMI\", \"IPMI\", \"PMI\"]\n",
    "for k in range(num_samples):\n",
    "    if len(set(Y_label['scp_codes'].iloc[k].keys()) & set(danger_list)) > 0:\n",
    "        Y[k] = 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split into train/test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check label distribution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_num_total = Y_train.shape[0]\n",
    "train_num_mi = np.sum(Y_train)\n",
    "train_mi_ratio = train_num_mi / train_num_total\n",
    "print('MI/non-MI proportion in train set: %.2f' % (100 * train_mi_ratio) + '%')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plot = plt.bar(x=[0, 1], height=[train_num_total - train_num_mi, train_num_mi], color=['b', 'r'], tick_label=['No MI', 'MI'])"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_num_total = Y_test.shape[0]\n",
    "test_num_mi = np.sum(Y_test)\n",
    "test_mi_ratio = test_num_mi / test_num_total\n",
    "print('MI/non-MI proportion in test set: %.2f' % (100.0 * test_mi_ratio) + '%')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plot = plt.bar(x=[0, 1], height=[test_num_total - test_num_mi, test_num_mi], color=['b', 'r'], tick_label=['No MI', 'MI'])"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Network Definition"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initializers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "weight_init = initializers.HeNormal()\n",
    "bias_init = initializers.Zeros()\n",
    "init = { 'kernel_initializer': weight_init, 'bias_initializer': bias_init }"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Residual block"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Residual(layers.Layer):\n",
    "    def __init__(self, last_num_filters, **kwargs):\n",
    "        super(Residual, self).__init__(**kwargs)\n",
    "        self.last_num_filters = last_num_filters\n",
    "\n",
    "        self.layer_1 = layers.Conv2D(last_num_filters + 64, (16, 1), activation=None, padding='same', **init)\n",
    "        self.layer_2 = layers.MaxPooling2D((2, 1)) # CHECK BECAUSE PAPER DOESN'T MENTION PRECISELY\n",
    "        self.layer_3 = layers.BatchNormalization(axis=[1,2]) #? axis = [1,2] to normalize the axis=0 (over the batch)\n",
    "        self.layer_4 = layers.Activation(activation='relu')\n",
    "        self.layer_5 = layers.Dropout(dropout_p) # To prevent overfit\n",
    "        self.layer_6 = layers.Conv2D(last_num_filters + 64, (16, 1), activation=None, padding='same', **init)\n",
    "        self.layer_7 = layers.MaxPooling2D((2, 1)) # CHECK BECAUSE PAPER DOESN'T MENTION PRECISELY\n",
    "        self.layer_8 = layers.BatchNormalization(axis=[1,2]) #? axis = [1,2] to normalize the axis=0 (over the batch)\n",
    "        self.layer_9 = layers.Activation(activation='relu')\n",
    "        self.layer_10 = layers.Dropout(dropout_p) # To prevent overfit\n",
    "\n",
    "        self.layer_11 = layers.Conv2D(last_num_filters + 64, (1, 1), activation=None, padding='same', **init)\n",
    "        self.layer_12 = layers.MaxPooling2D((4, 1)) # CHECK BECAUSE PAPER DOESN'T MENTION PRECISELY\n",
    "        self.layer_13 = layers.BatchNormalization(axis=[1,2]) #? axis = [1,2] to normalize the axis=0 (over the batch)\n",
    "        self.layer_14 = layers.Activation(activation='relu')\n",
    "        self.layer_15 = layers.Dropout(dropout_p) # To prevent overfit\n",
    "\n",
    "    def call(self, x):\n",
    "        # the residual block using Keras functional API\n",
    "        x_backup = x\n",
    "        last_num_filters = self.last_num_filters\n",
    "\n",
    "        x = self.layer_1(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        x = self.layer_4(x)\n",
    "        x = self.layer_5(x)\n",
    "        x = self.layer_6(x)\n",
    "        x = self.layer_7(x)\n",
    "        x = self.layer_8(x)\n",
    "        x = self.layer_9(x)\n",
    "        x = self.layer_10(x)\n",
    "\n",
    "        x_backup = self.layer_11(x_backup)\n",
    "        x_backup = self.layer_12(x_backup)\n",
    "        x_backup = self.layer_13(x_backup)\n",
    "        x_backup = self.layer_14(x_backup)\n",
    "        x_backup = self.layer_15(x_backup)\n",
    "        \n",
    "        x = layers.Add()([x,x_backup])\n",
    "\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dropout_p = 0.1\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(64, (16, 1), activation=None, input_shape=(4096, 12, 1), **init))\n",
    "model.add(layers.BatchNormalization(axis=[1, 2])) #? axis = [1,2] to normalize the axis=0 (over the batch)\n",
    "model.add(layers.Activation(activation='relu'))\n",
    "model.add(layers.Dropout(dropout_p)) # to prevent overfit\n",
    "\n",
    "last_num_filters = 64\n",
    "model.add(Residual(last_num_filters))\n",
    "last_num_filters += 64\n",
    "model.add(Residual(last_num_filters))\n",
    "last_num_filters += 64\n",
    "model.add(Residual(last_num_filters))\n",
    "last_num_filters += 64\n",
    "model.add(Residual(last_num_filters))\n",
    "last_num_filters += 64\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(1, activation='sigmoid', **init))\n",
    "\n",
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compile"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Credits to https://datascience.stackexchange.com/a/45166\n",
    "auc_metric = tf.keras.metrics.AUC(num_thresholds=200)\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compile model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auc_metric, f1_m,precision_m, recall_m])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check whether we're training on a GPU or not\n",
    "tf.test.is_gpu_available()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.fit(X_train, Y_train, epochs=40, batch_size=16, validation_data=(X_test, Y_test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e8fe47fbcf0df5fb1553eccd10fe68adaaecd5a77378650c47b51c76c16f447a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}